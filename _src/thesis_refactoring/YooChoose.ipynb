{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# Data preprocessing methods\n",
    "import DataHandler\n",
    "from DataHandler import *\n",
    "if sys.version_info > (3, 0):\n",
    "    importlib.reload(DataHandler)\n",
    "\n",
    "# Input object classes\n",
    "import SessionData\n",
    "from SessionData import *\n",
    "if sys.version_info > (3, 0):\n",
    "    importlib.reload(SessionData)\n",
    "\n",
    "import ItemData\n",
    "from ItemData import *\n",
    "if sys.version_info > (3, 0):\n",
    "    importlib.reload(ItemData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to create small sample dataset, otherwise False\n",
    "use_sample = False\n",
    "use_subset = False\n",
    "subset_size = 10000\n",
    "\n",
    "dir = os.getcwd()\n",
    "\n",
    "filename_sample_clicks = os.path.join(dir, '..', 'data', 'yoochoose-clicks-sample.dat')\n",
    "filename_sample_buys = os.path.join(dir, '..', 'data', 'yoochoose-buys-sample.dat')\n",
    "\n",
    "filename_clicks = os.path.join(dir, '..', 'data', 'yoochoose-clicks_100k.dat')\n",
    "filename_buys = os.path.join(dir, '..', 'data', 'yoochoose-buys_100k.dat')\n",
    "\n",
    "filename_test = os.path.join(dir, '../data/yoochoose-test.dat')\n",
    "\n",
    "if use_sample:\n",
    "    print ('Working with test dataset')\n",
    "\n",
    "    train_buys, valid_buys, test_buys = create_dataset_buys(filename_sample_buys)\n",
    "    train_clicks, valid_clicks, test_clicks = create_dataset_clicks(filename_sample_clicks)\n",
    "\n",
    "else:\n",
    "    print ('Working with real dataset')\n",
    "\n",
    "    all_buys, train_buys, valid_buys, test_buys = create_dataset_buys(filename_buys)\n",
    "    train_clicks, valid_clicks, test_clicks = create_dataset_clicks(filename_clicks)\n",
    "\n",
    "if use_subset:\n",
    "    print ('Working with subset')\n",
    "\n",
    "    train_buys = train_buys[:subset_size]\n",
    "    train_clicks = train_clicks[:subset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------  Creating input  ------- \\n')\n",
    "\n",
    "# Create input to the network\n",
    "# train_sessions = Session.create_sessions_from_clicks(train_clicks)\n",
    "# print('Created Sessions')\n",
    "# train_session_objects = SessionObject.create_session_object_list(train_sessions)\n",
    "# print('Created Session Objects')\n",
    "# train_vectors = SessionObject.create_input_vectors(train_session_objects, info=False)\n",
    "# print('Created Input Vectors')\n",
    "# train_labels = create_labels(all_buys, train_session_objects, info=False)\n",
    "# train_labels = reformat(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_dict, item_dict = DataHandler.structure_raw_data(train_clicks, all_buys)\n",
    "print('Created [SessionData dictionary, ItemData dictionary] from dataset rows.')\n",
    "\n",
    "input_vectors, output_vectors, _ = SessionData.SessionData.create_input_output_vectors(session_dict, info=True)\n",
    "print('Finished creating [input vectors, output vectors].')\n",
    "\n",
    "# Balancing dataset because of incredible ratio of buy sessions and non-buy sessions\n",
    "# Final ratio - 1:1\n",
    "train_vectors, train_labels = oversample_dataset(input_vectors, output_vectors, info=True)\n",
    "\n",
    "train_batches_vectors = []\n",
    "train_batches_labels = []\n",
    "batch_size = 128\n",
    "\n",
    "# Create training batches of vectors and labels\n",
    "for index in range(0, len(train_vectors) / batch_size):\n",
    "    train_batches_vectors.append(train_vectors[batch_size * index:batch_size * (index + 1)])\n",
    "    train_batches_labels.append(train_labels[batch_size * index:batch_size * (index + 1)])\n",
    "    \n",
    "print('Created train dataset\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sessions = Session.create_sessions_from_clicks(valid_clicks)\n",
    "valid_session_objects = SessionObject.create_session_object_list(valid_sessions)\n",
    "valid_vectors = SessionObject.create_input_vectors(valid_session_objects, info=False)\n",
    "valid_labels = create_labels(all_buys, valid_session_objects, info=False)\n",
    "valid_labels = reformat(valid_labels)\n",
    "\n",
    "print('Created validation dataset\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sessions = Session.create_sessions_from_clicks(test_clicks)\n",
    "test_session_objects = SessionObject.create_session_object_list(test_sessions)\n",
    "test_vectors = SessionObject.create_input_vectors(test_session_objects, info=False)\n",
    "test_labels = create_labels(all_buys, test_session_objects, info=False)\n",
    "test_labels = reformat(test_labels)\n",
    "\n",
    "print('Created test dataset\\n')\n",
    "\n",
    "print('--------  Dimensions  -------- \\n')\n",
    "\n",
    "print(' Train dataset:        %d %d   ' % (len(train_vectors), len(train_vectors[0])))\n",
    "print(' Train labels:         %d %d \\n' % (len(train_labels), len(train_labels[0])))\n",
    "\n",
    "print(' Validation dataset:   %d %d   ' % (len(valid_vectors), len(valid_vectors[0])))\n",
    "print(' Validation labels:    %d %d \\n' % (len(valid_labels), len(valid_labels[0])))\n",
    "\n",
    "print(' Test dataset:         %d %d   ' % (len(test_vectors), len(test_vectors[0])))\n",
    "print(' Test labels:          %d %d \\n' % (len(test_labels), len(test_labels[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Variables.\n",
    "    num_of_labels = 2\n",
    "    learning_rate = 0.001\n",
    "    hidden_size = 128\n",
    "\n",
    "    # Input data. For the training data, use a placeholder that will be fed at run time with a training minibatch\n",
    "    tf_train_vectors = tf.placeholder(tf.float32, shape=(batch_size, len(train_vectors[0])))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, len(train_labels[0])))\n",
    "    tf_valid_vectors = tf.constant(valid_vectors)\n",
    "    tf_test_vectors = tf.constant(test_vectors)\n",
    "\n",
    "    weights_h1 = tf.Variable(tf.truncated_normal([len(train_vectors[0]), hidden_size]))\n",
    "    biases_h1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "    h1 = tf.nn.relu(tf.matmul(tf_train_vectors, weights_h1) + biases_h1)\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_size, num_of_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_of_labels]))\n",
    "\n",
    "    # Training computation\n",
    "    logits = tf.matmul(h1, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    # optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(\n",
    "            tf.nn.relu(\n",
    "                tf.matmul(tf_valid_vectors, weights_h1) + biases_h1), weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(\n",
    "            tf.nn.relu(\n",
    "                tf.matmul(tf_test_vectors, weights_h1) + biases_h1), weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute session and get results of train and test datasets \n",
    "\n",
    "Each batch should (in theory):\n",
    "\tIncrease the accuracy\n",
    "\tDecrease the loss\n",
    "\n",
    "After finishing all batches, test dataset is ran through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    last_validation_accuracy = 0.0\n",
    "\n",
    "    stop_training = 0\n",
    "    print(len(train_batches_vectors))\n",
    "\n",
    "    while stop_training < 3:\n",
    "        for batch_index in range(len(train_batches_vectors)):\n",
    "            # Generate a batch.\n",
    "            batch_vectors = train_batches_vectors[batch_index]\n",
    "            batch_labels = train_batches_labels[batch_index]\n",
    "\n",
    "            # Feed the dictionary and start training\n",
    "            feed_dict = {tf_train_vectors: batch_vectors, tf_train_labels: batch_labels}\n",
    "\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction],\n",
    "                feed_dict=feed_dict\n",
    "            )\n",
    "\n",
    "            # print(\"Batch loss at batch %d: %f\" % (batch_index, l))\n",
    "            # print(\"Batch accuracy: %.1f%%\" %\n",
    "            #      accuracy(predictions, batch_labels))\n",
    "\n",
    "            print(\"Validate accuracy: %.1f%%\\n\" %\n",
    "                  accuracy(valid_prediction.eval(), valid_labels, precision_print=False))\n",
    "        # with open(\"validations.txt\", \"a\") as myfile:\n",
    "        #                 textfloat = accuracy(valid_prediction.eval(), valid_labels, precision_print=False)\n",
    "        #                 myfile.write(\"%.3f\\n\" % textfloat)\n",
    "\n",
    "        validation_accuracy = accuracy(\n",
    "            valid_prediction.eval(), valid_labels, precision_print=True)\n",
    "\n",
    "        # Count the difference of validation set accuracy\n",
    "        diff = validation_accuracy - last_validation_accuracy\n",
    "        if diff < 1.0:\n",
    "            stop_training += 1\n",
    "            print('Accuracy improvement only %.3f, stop training increased to %d' % (diff, stop_training))\n",
    "        else:\n",
    "            print('Stop training reset')\n",
    "            stop_training = 0\n",
    "\n",
    "        last_validation_accuracy = validation_accuracy\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" %\n",
    "          accuracy(test_prediction.eval(), test_labels, precision_print=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}